# **M贸dulo 2: Limpieza y Transformaci贸n de Datos**

La limpieza y transformaci贸n de datos es una de las etapas m谩s cr铆ticas en cualquier proyecto de ciencia de datos. A menudo, los datos brutos que obtenemos no est谩n listos para ser analizados directamente. Contienen errores, inconsistencias, valores faltantes y redundancias que pueden sesgar los resultados o dificultar el an谩lisis. En este m贸dulo, profundizaremos en cada uno de los aspectos clave de la limpieza y transformaci贸n de datos, proporcionando ejemplos pr谩cticos y explicaciones detalladas.

---

## **1. Ciclo de Vida de un Proyecto de Ciencia de Datos**

Antes de profundizar en la limpieza y transformaci贸n de datos, es importante entender c贸mo estas actividades se integran en el ciclo de vida completo de un proyecto de ciencia de datos:

1. **Definici贸n del problema**: Identificar claramente el objetivo del proyecto.
2. **Recolecci贸n de datos**: Obtener los datos necesarios desde diversas fuentes.
3. **Limpieza y preparaci贸n de datos**: Limpiar y transformar los datos para hacerlos aptos para el an谩lisis.
4. **An谩lisis exploratorio**: Explorar patrones y tendencias en los datos.
5. **Modelado**: Construir modelos predictivos o descriptivos.
6. **Evaluaci贸n**: Validar el rendimiento del modelo.
7. **Despliegue**: Implementar el modelo en un entorno real.
8. **Monitoreo**: Supervisar el rendimiento continuo del modelo.

En este m贸dulo, nos enfocaremos en la tercera etapa: **Limpieza y Preparaci贸n de Datos**.

---

## **2. Definici贸n del Problema**

El primer paso antes de comenzar con la limpieza de datos es **definir claramente el problema** que queremos resolver. Esto implica:

- **Entender el contexto del negocio**: 驴Qu茅 preguntas estamos tratando de responder? Por ejemplo, si trabajamos en marketing, podr铆amos estar interesados en segmentar a los clientes para ofrecerles productos personalizados.
  
- **Identificar los datos necesarios**: Una vez definido el problema, debemos identificar qu茅 tipo de datos necesitaremos para resolverlo. Esto podr铆a incluir datos demogr谩ficos, transacciones, interacciones en redes sociales, etc.

- **Establecer m茅tricas de 茅xito**: 驴C贸mo mediremos el 茅xito del proyecto? Por ejemplo, si estamos construyendo un modelo predictivo, podr铆amos medir su precisi贸n, recall, F1-score, etc.

### **Ejemplo Pr谩ctico**
Supongamos que estamos trabajando en un proyecto de predicci贸n de churn (abandono) de clientes en una empresa de telecomunicaciones. El problema podr铆a definirse como:

- **Objetivo**: Predecir qu茅 clientes tienen mayor probabilidad de abandonar el servicio en los pr贸ximos meses.
- **Datos necesarios**: Historial de uso del cliente, facturaci贸n, soporte t茅cnico, quejas, etc.
- **M茅trica de 茅xito**: Precisi贸n del modelo en predecir correctamente el churn.

---

## **3. Recolecci贸n de Datos**

Una vez definido el problema, el siguiente paso es **recolectar los datos** necesarios. Los datos pueden provenir de diversas fuentes, como:

- **Bases de datos SQL**: Usualmente utilizadas para almacenar datos estructurados.
- **APIs**: Servicios web que proporcionan datos en tiempo real.
- **Archivos CSV/Excel**: Comunes en proyectos peque帽os o medianos.
- **Web scraping**: Extracci贸n de datos de sitios web.
- **Sensores/IoT**: Datos generados por dispositivos conectados.

### **Herramientas para la Recolecci贸n de Datos**
- **Pandas**: Para leer archivos CSV, Excel, JSON, etc.
- **SQLAlchemy**: Para conectarse a bases de datos SQL.
- **Requests**: Para interactuar con APIs.
- **BeautifulSoup/Scrapy**: Para realizar web scraping.

### **Ejemplo Pr谩ctico**
Supongamos que tenemos un archivo CSV con datos de clientes de telecomunicaciones. Podemos cargarlo en Python usando Pandas:

```python
import pandas as pd

# Cargar datos desde un archivo CSV
df = pd.read_csv('datos_clientes.csv')

# Mostrar las primeras filas del DataFrame
print(df.head())
```

---

## **4. Manejo de Valores Faltantes y Duplicados**

Uno de los problemas m谩s comunes en los datos es la presencia de **valores faltantes** y **registros duplicados**. Estos problemas deben abordarse antes de continuar con el an谩lisis.

### **Valores Faltantes**
Los valores faltantes pueden aparecer por varias razones:
- Errores en la recolecci贸n de datos.
- Falta de respuesta en encuestas.
- Datos perdidos durante la transferencia.

#### **Estrategias para manejar valores faltantes**
1. **Eliminar registros con valores faltantes**:
   ```python
   df.dropna(inplace=True)
   ```
   Esto elimina todas las filas que contienen al menos un valor faltante.

2. **Rellenar valores faltantes**:
   - Con un valor constante:
     ```python
     df.fillna(0, inplace=True)
     ```
   - Con la media, mediana o moda:
     ```python
     df['edad'].fillna(df['edad'].mean(), inplace=True)
     ```

3. **Interpolaci贸n**:
   La interpolaci贸n es 煤til cuando los datos tienen una secuencia temporal o espacial.
   ```python
   df['valor'] = df['valor'].interpolate()
   ```

### **Registros Duplicados**
Los registros duplicados pueden distorsionar el an谩lisis, especialmente en modelos de aprendizaje autom谩tico.

#### **Eliminar duplicados**
```python
df.drop_duplicates(inplace=True)
```

---

## **5. Filtrado y Selecci贸n de Datos**

A menudo, no necesitamos todos los datos disponibles. Es posible que solo necesitemos ciertas columnas o filas que cumplan con ciertas condiciones.

### **Filtrado de Datos**
Podemos filtrar datos basados en condiciones espec铆ficas. Por ejemplo, si queremos seleccionar solo los clientes mayores de 30 a帽os:

```python
df_filtrado = df[df['edad'] > 30]
```

### **Selecci贸n de Columnas**
Si solo necesitamos ciertas columnas, podemos seleccionarlas:

```python
df_seleccionado = df[['nombre', 'edad', 'ingresos']]
```

---

## **6. Transformaci贸n de Columnas**

La transformaci贸n de columnas implica modificar los datos existentes o crear nuevas caracter铆sticas (features) que puedan mejorar el rendimiento del modelo.

### **Crear Nuevas Columnas**
Podemos crear nuevas columnas basadas en c谩lculos o transformaciones de columnas existentes. Por ejemplo, si queremos calcular el ingreso mensual promedio:

```python
df['ingreso_mensual'] = df['ingreso_anual'] / 12
```

### **Modificar Columnas Existentes**
Podemos aplicar funciones a columnas existentes para modificar sus valores. Por ejemplo, convertir todos los nombres a may煤sculas:

```python
df['nombre'] = df['nombre'].apply(lambda x: x.upper())
```

---

## **7. Uni贸n y Concatenaci贸n de DataFrames**

En muchos casos, necesitaremos combinar m煤ltiples conjuntos de datos para obtener una vista completa.

### **Uni贸n de DataFrames**
La uni贸n permite combinar dos DataFrames basados en una columna com煤n (clave).

```python
df_unido = pd.merge(df1, df2, on='id_cliente')
```

### **Concatenaci贸n de DataFrames**
La concatenaci贸n permite apilar DataFrames vertical u horizontalmente.

```python
df_concatenado = pd.concat([df1, df2], axis=0)  # Apilado verticalmente
```

---

## **8. Manejo de Conjuntos de Datos**

El manejo de conjuntos de datos implica trabajar con diferentes tipos de datos, tanto reales como sint茅ticos.

### **Conjuntos de Datos Reales**
Los datos reales provienen de fuentes del mundo real, como bases de datos empresariales, encuestas, sensores, etc. Estos datos suelen ser ruidosos y requieren limpieza extensa.

### **Conjuntos de Datos Sint茅ticos**
Los datos sint茅ticos son generados artificialmente para pruebas o simulaciones. Son 煤tiles cuando no se dispone de datos reales o cuando se quiere proteger la privacidad.

#### **Generaci贸n de Datos Sint茅ticos**
Podemos generar datos sint茅ticos usando bibliotecas como `numpy` o `faker`.

```python
import numpy as np
import pandas as pd

# Generar datos sint茅ticos
data = {
    'edad': np.random.randint(18, 65, size=100),
    'ingreso': np.random.normal(50000, 15000, size=100)
}
df_sintetico = pd.DataFrame(data)
```

---

## **9. Conjuntos de Datos Reales y Sint茅ticos**

Finalmente, es importante destacar que los **conjuntos de datos reales** suelen ser m谩s complejos y desordenados que los sint茅ticos. Sin embargo, los datos sint茅ticos son 煤tiles para probar algoritmos y modelos en un entorno controlado.

### **Ejemplo Pr谩ctico**
Supongamos que estamos trabajando con un conjunto de datos real de clientes de telecomunicaciones. Podr铆amos seguir estos pasos:

1. **Cargar los datos** desde un archivo CSV.
2. **Limpiar los datos**: Eliminar valores faltantes, duplicados y corregir inconsistencias.
3. **Transformar los datos**: Crear nuevas columnas o modificar las existentes.
4. **Unir datos adicionales**: Si tenemos informaci贸n adicional sobre los clientes, como historial de compras, podemos unirla al conjunto principal.
5. **Guardar el conjunto limpio** para su posterior an谩lisis.


---

# **Ciclo de Vida de un Proyecto de Ciencia de Datos: An谩lisis Exploratorio, Modelado, Evaluaci贸n, Despliegue y Monitoreo**

En el m贸dulo anterior, profundizamos en la limpieza y transformaci贸n de datos, una etapa crucial para preparar los datos antes del an谩lisis. Ahora, continuaremos explorando las siguientes fases del ciclo de vida de un proyecto de ciencia de datos: **An谩lisis Exploratorio**, **Modelado**, **Evaluaci贸n**, **Despliegue** y **Monitoreo**. Cada una de estas etapas es fundamental para garantizar que el proyecto sea exitoso y que los modelos construidos sean 煤tiles y confiables.

---

## **1. An谩lisis Exploratorio de Datos (EDA)**

El **An谩lisis Exploratorio de Datos (EDA)** es una fase cr铆tica en cualquier proyecto de ciencia de datos. Su objetivo es comprender mejor los datos, identificar patrones, tendencias y relaciones entre variables, as铆 como detectar anomal铆as o problemas potenciales en los datos. Este proceso ayuda a formular hip贸tesis y guiar el desarrollo del modelo.

### **Objetivos del EDA**
- **Conocer la distribuci贸n de los datos**: Entender c贸mo est谩n distribuidas las variables num茅ricas y categ贸ricas.
- **Identificar valores at铆picos (outliers)**: Detectar puntos de datos que se desv铆an significativamente del resto.
- **Explorar correlaciones**: Identificar relaciones entre variables.
- **Detectar patrones y tendencias**: Observar si hay patrones temporales, espaciales o de otro tipo.
- **Validar suposiciones**: Asegurarse de que los datos cumplen con las suposiciones necesarias para el modelado.

### **Herramientas Comunes para el EDA**
- **Pandas**: Para manipulaci贸n y an谩lisis b谩sico de datos.
- **Matplotlib/Seaborn**: Para visualizaci贸n de datos.
- **NumPy**: Para c谩lculos estad铆sticos.
- **Scipy**: Para pruebas estad铆sticas avanzadas.

### **Pasos Clave en el EDA**

#### **1.1 Resumen Estad铆stico**
El primer paso es obtener un resumen estad铆stico de los datos. Esto incluye medidas como media, mediana, desviaci贸n est谩ndar, m铆nimo y m谩ximo.

```python
import pandas as pd

# Resumen estad铆stico
print(df.describe())
```

#### **1.2 Visualizaci贸n de Datos**
La visualizaci贸n es clave para entender la distribuci贸n de los datos y las relaciones entre variables.

- **Histogramas**: Para ver la distribuci贸n de una variable num茅rica.
  ```python
  import matplotlib.pyplot as plt

  df['edad'].hist(bins=30)
  plt.title('Distribuci贸n de Edades')
  plt.xlabel('Edad')
  plt.ylabel('Frecuencia')
  plt.show()
  ```

- **Diagramas de dispersi贸n**: Para explorar relaciones entre dos variables.
  ```python
  plt.scatter(df['edad'], df['ingresos'])
  plt.title('Relaci贸n entre Edad e Ingresos')
  plt.xlabel('Edad')
  plt.ylabel('Ingresos')
  plt.show()
  ```

- **Mapas de calor**: Para visualizar correlaciones entre variables.
  ```python
  import seaborn as sns

  corr = df.corr()
  sns.heatmap(corr, annot=True, cmap='coolwarm')
  plt.title('Mapa de Calor de Correlaciones')
  plt.show()
  ```

#### **1.3 Detecci贸n de Outliers**
Los outliers pueden distorsionar el an谩lisis y deben ser tratados cuidadosamente.

- **Boxplots**: Para identificar valores at铆picos.
  ```python
  sns.boxplot(x=df['ingresos'])
  plt.title('Boxplot de Ingresos')
  plt.show()
  ```

#### **1.4 An谩lisis de Variables Categ贸ricas**
Para variables categ贸ricas, podemos usar gr谩ficos de barras o tablas de frecuencia.

```python
df['categoria'].value_counts().plot(kind='bar')
plt.title('Distribuci贸n de Categor铆as')
plt.show()
```

---

## **2. Modelado**

Una vez que hemos comprendido los datos mediante el EDA, el siguiente paso es **construir modelos predictivos o descriptivos**. El modelado implica seleccionar algoritmos adecuados, entrenarlos con los datos y ajustarlos para obtener el mejor rendimiento posible.

### **Tipos de Modelos**
- **Modelos Predictivos**: Predicen una variable objetivo bas谩ndose en otras variables (regresi贸n, clasificaci贸n).
- **Modelos Descriptivos**: Explican patrones o relaciones en los datos (clustering, reglas asociativas).

### **Pasos en el Modelado**

#### **2.1 Divisi贸n de Datos**
Es importante dividir los datos en conjuntos de entrenamiento y prueba para evaluar el rendimiento del modelo.

```python
from sklearn.model_selection import train_test_split

X = df.drop('target', axis=1)  # Variables independientes
y = df['target']               # Variable dependiente

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

#### **2.2 Selecci贸n del Algoritmo**
Dependiendo del problema, podemos elegir diferentes algoritmos:
- **Regresi贸n Lineal**: Para problemas de regresi贸n.
- **rboles de Decisi贸n**: Para problemas de clasificaci贸n o regresi贸n.
- **K-Means**: Para clustering.
- **Redes Neuronales**: Para problemas m谩s complejos.

#### **2.3 Entrenamiento del Modelo**
Usamos el conjunto de entrenamiento para entrenar el modelo.

```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
```

#### **2.4 Predicci贸n**
Una vez entrenado, el modelo puede hacer predicciones sobre nuevos datos.

```python
y_pred = model.predict(X_test)
```

---

## **3. Evaluaci贸n**

La **evaluaci贸n** del modelo es crucial para determinar su rendimiento y fiabilidad. Dependiendo del tipo de problema (clasificaci贸n, regresi贸n, etc.), usamos diferentes m茅tricas.

### **M茅tricas Comunes**
- **Clasificaci贸n**:
  - **Accuracy**: Proporci贸n de predicciones correctas.
  - **Precision, Recall, F1-Score**: Para problemas de clasificaci贸n desbalanceados.
  
  ```python
  from sklearn.metrics import classification_report

  print(classification_report(y_test, y_pred))
  ```

- **Regresi贸n**:
  - **Mean Absolute Error (MAE)**: Error absoluto medio.
  - **Mean Squared Error (MSE)**: Error cuadr谩tico medio.
  - **R虏 Score**: Coeficiente de determinaci贸n.

  ```python
  from sklearn.metrics import mean_squared_error, r2_score

  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)

  print(f'MSE: {mse}, R虏: {r2}')
  ```

---

## **4. Despliegue**

El **despliegue** implica implementar el modelo en un entorno real donde pueda interactuar con usuarios o sistemas. Esto puede incluir la creaci贸n de APIs, integraci贸n con aplicaciones web o m贸viles, o incluso la automatizaci贸n de decisiones.

### **Pasos en el Despliegue**
1. **Exportar el modelo**: Guardar el modelo entrenado para su uso posterior.
   ```python
   import joblib

   joblib.dump(model, 'modelo_entrenado.pkl')
   ```

2. **Crear una API**: Usar frameworks como Flask o FastAPI para exponer el modelo como un servicio.
   ```python
   from flask import Flask, request, jsonify
   import joblib

   app = Flask(__name__)
   model = joblib.load('modelo_entrenado.pkl')

   @app.route('/predict', methods=['POST'])
   def predict():
       data = request.json
       prediction = model.predict([data['features']])
       return jsonify({'prediction': prediction.tolist()})

   if __name__ == '__main__':
       app.run(debug=True)
   ```

3. **Integraci贸n con sistemas existentes**: Conectar el modelo con bases de datos, sistemas ERP, CRM, etc.

---

## **5. Monitoreo**

Finalmente, una vez que el modelo est谩 en producci贸n, es crucial **monitorear su rendimiento** continuamente. Los modelos pueden degradarse con el tiempo debido a cambios en los datos o en el entorno.

### **Indicadores de Monitoreo**
- **Drift de datos**: Cambios en la distribuci贸n de los datos de entrada.
- **Precisi贸n del modelo**: Verificar si las predicciones siguen siendo precisas.
- **Latencia**: Tiempo que tarda el modelo en hacer predicciones.

### **Herramientas de Monitoreo**
- **Prometheus/Grafana**: Para monitorear m茅tricas en tiempo real.
- **MLflow**: Para gestionar ciclos de vida de modelos y monitorear su rendimiento.

---

# **Explorando los Secretos Ocultos de Tus Datos: M谩s All谩 de la Limpieza B谩sica**  

Imagina que tienes un mapa del tesoro, pero algunas partes est谩n borrosas o tienen s铆mbolos extra帽os que no entiendes. As铆 son los datos sin un buen manejo de *outliers* y validaci贸n de formatos: 隆pierdes el rumbo! Aqu铆 te revelamos c贸mo dominar estas habilidades para que tus an谩lisis brillen como oro.  

---

## **1. Los Rebeldes del Dataset: 驴Qu茅 Hacer con los Outliers?**  

### **驴Por qu茅 son un problema?**  
Los outliers son como esos amigos que siempre llegan tarde o temprano a una reuni贸n: rompen el ritmo. En datos, pueden distorsionar promedios, inflar errores en modelos y ocultar patrones reales.  

### **Cazando Outliers con Boxplots**  
Visualiza tus datos con gr谩ficos de caja (*boxplots*). As铆 detectar谩s valores extremos f谩cilmente:  
```python  
import seaborn as sns  
sns.boxplot(x=df['ingresos'])  
plt.title("驴Qui茅n gana un mill贸n al mes?")  
plt.show()  
```  

### **T茅cnicas para Domesticarlos**  
- **Recorte (Trimming)**: Elimina los valores extremos.  
  ```python  
  df = df[(df['ingresos'] < 200000) & (df['ingresos'] > 1000)]  
  ```  
- **Imputaci贸n inteligente**: Rellena outliers con la mediana o la media.  
  ```python  
  mediana = df['ingresos'].median()  
  df['ingresos'] = df['ingresos'].apply(lambda x: mediana if x > 200000 else x)  
  ```  

**Actividad pr谩ctica (20 min):**  
Usa un dataset de sueldos (ejemplo: `sueldos.csv`) y aplica un boxplot. Luego, decide si recortar o imputar los outliers.  

---

## **2. El Detective de Formatos: Validando Tus Datos**  

### **驴Por qu茅 validar?**  
Un n煤mero escrito como texto o una fecha en formato incorrecto son como llaves que no abren puertas: bloquean tu an谩lisis.  

### **Trucos para Verificar Formatos**  
- **Fechas**: Convierte cadenas a formato fecha con Pandas.  
  ```python  
  df['fecha_nacimiento'] = pd.to_datetime(df['fecha_nacimiento'], errors='coerce')  
  ```  
- **N煤meros**: Aseg煤rate de que las columnas num茅ricas no escondan texto.  
  ```python  
  df['edad'] = pd.to_numeric(df['edad'], errors='coerce')  
  ```  
- **Expresiones regulares**: Busca patrones espec铆ficos (ej: correos electr贸nicos v谩lidos).  
  ```python  
  import re  
  df['correo_valido'] = df['correo'].apply(lambda x: bool(re.match(r'[\w.-]+@[\w.-]+', x)))  
  ```  

### **Ejemplo de Validaci贸n Creativa**  
Imagina que tienes una columna `tel茅fono` con mezcla de n煤meros y letras. Usa una m谩scara para limpiarla:  
```python  
df['tel茅fono'] = df['tel茅fono'].str.replace(r'\D', '', regex=True)  # Elimina todo lo que no sea n煤mero  
```  

**Actividad pr谩ctica (30 min):**  
Descarga un dataset con errores comunes (ej: `clientes_desordenados.csv`) y corrige:  
1. Convierte una columna de texto a n煤meros.  
2. Extrae a帽os de una columna de fechas mal formateadas (ej: "15/03/2023" vs "Marzo-2023").  

---

## **3. Conclusi贸n: Tu Kit de Supervivencia en Ciencia de Datos**  

Dominar el manejo de outliers y la validaci贸n de formatos es como tener un botiqu铆n de primeros auxilios para datos. Te permite:  
- **Tomar decisiones m谩s precisas** (sin outliers enga帽osos).  
- **Evitar errores absurdos** (como sumar n煤meros escritos como texto).  

**Reto final:**  
Combina ambos temas: usa un dataset con outliers *y* formatos inconsistentes (ej: `datos_caoticos.xlsx`). Limpia, valida y comparte tus hallazgos en un informe de tres frases.  

--- 

驴Listo para convertirte en el h茅roe de tus datos? Ω锔 Con estas herramientas, ning煤n dataset se te resistir谩.